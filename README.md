# Logistic Regression from Scratch üìö

This project is a pure NumPy implementation of logistic regression, built entirely from scratch to understand the core mathematical operations behind neural networks.

The goal is to manually implement each part ‚Äî forward pass, loss calculation, backpropagation, and parameter updates ‚Äî without relying on any machine learning libraries.

---

## Features
- Manual implementation of:
  - Forward propagation
  - Cross-entropy loss calculation
  - Gradient computation
  - Parameter updates via gradient descent
- Only NumPy used ‚Äî no high-level ML libraries (e.g., Scikit-Learn, TensorFlow, PyTorch)
- Clear separation between training and prediction steps
- Structured for easy expansion into more complex models (e.g., neural nets)

---

## Project Structure
- `LogisticRegressionFromScratch.ipynb` ‚Äî Jupyter Notebook containing the full model code and explanations.
- (Optional) `logistic_regression.py` ‚Äî Can be modularized into a script if needed later.

---

## Technologies Used
- **Python 3.x**
- **NumPy**

---

## Key Learnings
- Understanding how logistic regression works under the hood
- Building intuition for forward and backward passes
- Foundational knowledge for developing neural networks and deep learning models

---

## License
This project is open-source for educational purposes under the [MIT License](LICENSE).

---

Made with üß† and üõ†Ô∏è to dive deeper into machine learning fundamentals.
